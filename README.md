# Basic Transformer Model
# Transformer from Scratch for Abstractive Summarization

This project implements a Transformer-based encoder-decoder model **from scratch** using PyTorch, designed for **abstractive text summarization** on the [CNN/DailyMail dataset](https://huggingface.co/datasets/cnn_dailymail). It includes a complete pipeline for preprocessing, model training, beam search inference, and BLEU score evaluation.

---

## ğŸ“Œ Key Features

- ğŸ”§ Implemented the Transformer architecture from scratch (no HuggingFace or pretrained models).
- âœ‚ï¸ Cleaned and tokenized dataset manually, including:
  - Lowercasing
  - Removing punctuation and numbers
  - Filtering short samples
  - Padding/truncating to fixed lengths
- ğŸ§  Beam search decoder with repetition handling
- ğŸ“‰ Training and validation loss tracking
- ğŸ“ˆ Final **BLEU score: 22**
- ğŸ“ Python implementation + interactive training notebook

---

## ğŸ“Š Results

| Metric        | Value          |
|---------------|----------------|
| Training Loss | 24.429 â†’ 8.371 |
| Validation Loss | 9.386 â†’ 8.371 |
| BLEU Score    | 22             |
| Epochs        | 5              |
| Batch Size    | 32             |
| Dropout       | 0.5            |

---

## ğŸ§± Project Structure
.
â”œâ”€â”€ Processing_Summarizing_Datasets_From_Scratch.py # Custom text cleaning & tokenization
â”œâ”€â”€ Transformer_model.py # Transformer encoder/decoder classes
â”œâ”€â”€ summerizing.ipynb # Interactive training & testing notebook
â””â”€â”€ README.md

ğŸ“ˆ Example Loss Curve
![image](https://github.com/user-attachments/assets/311f22b4-70ac-4348-99aa-ec569bafb457)


ğŸ¤ Acknowledgments

    PyTorch

    Hugging Face Datasets

    CNN/DailyMail Dataset




