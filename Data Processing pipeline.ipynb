{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TgsGpkiAVu72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from string import punctuation\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade datasets\n",
        "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "oyWfbM98Zu7P",
        "outputId": "0f7d2a07-5e25-4f84-eda5-8d6a8d00fd25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2393c60e768c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install --upgrade datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_dailymail\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset['train']), len(dataset['test']), len(dataset['validation'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "4h3Z2W1YDeAc",
        "outputId": "0def64b0-a779-416a-8a50-d56f8035ff5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b4a547f102fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_articles = np.array(dataset['train'][:14000]['article'])\n",
        "train_summaries = np.array(dataset['train'][:14000]['highlights'])\n",
        "text_training = {'articles':train_articles,\n",
        "                 'summaries':train_summaries}\n",
        "\n",
        "validation_articles = np.array(dataset['validation'][:]['article'])\n",
        "validation_summaries = np.array(dataset['validation'][:]['highlights'])\n",
        "text_validation = {'articles':validation_articles,\n",
        "                   'summaries':validation_summaries}\n",
        "\n",
        "test_articles = np.array(dataset['test'][:]['article'])\n",
        "test_summaries = np.array(dataset['test'][:]['highlights'])\n",
        "text_test = {'articles':test_articles,\n",
        "             'summaries':test_summaries}"
      ],
      "metadata": {
        "id": "UzCI33qH-kek"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Processing"
      ],
      "metadata": {
        "id": "MBKWIzwPEOYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removing_unwanted_characters(text):\n",
        "    punctuations = punctuation.replace(\"'\",\"\")\n",
        "    punctuations = punctuation.replace(\".\",\"\")\n",
        "    punctuations = punctuation.replace(\",\",\"\")\n",
        "    punctuations += 'â€¢1234567890'\n",
        "\n",
        "    text = \"\".join([c for c in text if c not in punctuations])\n",
        "    text = text.split(\"\\n\")\n",
        "    text = \" \".join(text)\n",
        "    text = text.strip()\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    new_text = str(text[0])\n",
        "    for char in range(1, len(text)):\n",
        "        if (text[char] == '.' or text[char] == ',') and new_text[-1] != ' ':\n",
        "            new_text += ' '\n",
        "            new_text += text[char]\n",
        "        elif (new_text[-1] == '.' or new_text[-1] == ',') and text[char] != ' ':\n",
        "            new_text += ' '\n",
        "            new_text += text[char]\n",
        "        else:\n",
        "            new_text += text[char]\n",
        "\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "lqgTEUCHRjFJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing(data):\n",
        "    all_unique_words = []\n",
        "    for text in data:\n",
        "        all_unique_words.extend(text.split())\n",
        "    vocab = set(all_unique_words)\n",
        "    vocab = {word:ii for ii, word in enumerate(vocab, 3)}# '<pad>': 0, '<bos>': 1, '<eos>': 2\n",
        "\n",
        "    didicted_tokens = ['<pad>', '<bos>', '<eos>']\n",
        "    for i in range(3):\n",
        "        vocab[didicted_tokens[i]] = i\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "z_Lrr2iENzGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_words_to_tokens(data, vocab):\n",
        "    data_ints = []\n",
        "\n",
        "    for text in data:\n",
        "        data_ints.append([vocab.get(word) for word in text.split() if vocab.get(word) != None])\n",
        "        data_ints[-1].insert(0,1)\n",
        "        data_ints[-1].append(2)\n",
        "\n",
        "    size = int(data.shape[0] // 2)\n",
        "    input_data = data_ints[:size]\n",
        "    target_data = data_ints[size:]\n",
        "\n",
        "    return input_data, target_data"
      ],
      "metadata": {
        "id": "XHM1DgyGfQHn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_articles(data, input_data, target_data):\n",
        "    small_lenghts_indices = [index for index, text in enumerate(input_data) if len(text) < 100]\n",
        "    small_lenghts_indices = small_lenghts_indices[::-1]\n",
        "    for index in small_lenghts_indices:\n",
        "         input_data.pop(index)\n",
        "         target_data.pop(index)\n",
        "\n",
        "    size = int(data.shape[0] // 2)\n",
        "    small_lenghts_indices_targets = [length + size for length in small_lenghts_indices]\n",
        "    small_lenghts_indices = small_lenghts_indices + small_lenghts_indices_targets\n",
        "    data = np.delete(data, small_lenghts_indices, axis=0)\n",
        "\n",
        "    return data, input_data, target_data"
      ],
      "metadata": {
        "id": "vuQBgJe9gPq9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paddings(input_data, length):\n",
        "    size = len(input_data)\n",
        "    for i in range(size):\n",
        "        if len(input_data[i]) < length:\n",
        "            rem = length - len(input_data[i])\n",
        "            input_data[i].extend([0]*rem)\n",
        "\n",
        "        elif len(input_data[i]) > length:\n",
        "            input_data[i] = input_data[i][:length]\n",
        "\n",
        "    return input_data"
      ],
      "metadata": {
        "id": "yjzcDQMSVm21"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data, length_data, length_target, vocab):\n",
        "    keys = list(data.keys())\n",
        "    data = np.concatenate((data.get(keys[0]), data.get(keys[1])))\n",
        "    data = np.array([text.lower() for text in data])\n",
        "    items = data.shape[0]\n",
        "\n",
        "    for i in range(items):\n",
        "       data[i] += \" .\"\n",
        "       data[i] = removing_unwanted_characters(data[i])\n",
        "    if not vocab:\n",
        "        vocab = tokenizing(data)\n",
        "\n",
        "    input_data, target_data = convert_words_to_tokens(data, vocab)\n",
        "\n",
        "    data, input_data, target_data = remove_short_articles(data, input_data, target_data)\n",
        "\n",
        "    input_data = paddings(input_data, length_data)\n",
        "    target_data = paddings(target_data, length_target)\n",
        "\n",
        "    return data, torch.tensor(input_data), torch.tensor(target_data), vocab"
      ],
      "metadata": {
        "id": "rSQzF2ZgIOqV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train, train_data, train_target, vocab = preprocessing(text_training, 1000, 85, {})"
      ],
      "metadata": {
        "id": "w0WrrwEB-ll8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_val, val_data, val_target, vocab = preprocessing(text_validation, 1000, 85, vocab)"
      ],
      "metadata": {
        "id": "5GhLf8prjks-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test, test_data, test_target, vocab = preprocessing(text_test, 1000, 85, vocab)"
      ],
      "metadata": {
        "id": "DW2nhaerjld5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_target), type(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-58DOYLUhoPB",
        "outputId": "20ff86a3-c62d-4862-e7f4-8953db8c6c57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, dict)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(val_data), type(val_target), type(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uA_5u3AkQZE",
        "outputId": "2cc8bde4-0258-4c76-89f7-7c5305362519"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, dict)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(test_data), type(test_target), type(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eprIqlcokRMf",
        "outputId": "f2f26ad6-ca60-4ac9-f9c8-18bf2d5b7dd4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Tensor, torch.Tensor, dict)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, train_target.shape, len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YnHZIpdY_ug",
        "outputId": "055ac4cc-74cb-4424-e52e-7750f9428b33"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([13966, 1000]), torch.Size([13966, 85]), 113285)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.shape, val_target.shape, len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX80srqyLti4",
        "outputId": "fd9f3011-6004-4346-992d-892606a3fedd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([13357, 1000]), torch.Size([13357, 85]), 113285)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape, test_target.shape, len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZeJ_mpskeo1",
        "outputId": "8d46ca68-bc17-4214-a85c-548038938aee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([11473, 1000]), torch.Size([11473, 85]), 113285)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NB5I3P6mlGcy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "vocab_size = len(vocab)\n",
        "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=vocab['<pad>'])"
      ],
      "metadata": {
        "id": "QYS6VfxIy-84"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.cuda()\n",
        "embedding_layer = embedding_layer.cuda()"
      ],
      "metadata": {
        "id": "rrM5mxUK2HKs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = embedding_layer(train_data)\n",
        "validating_data = embedding_layer(val_data)\n",
        "testing_data = embedding_layer(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5yP5gqAaz5sC",
        "outputId": "78e9c960-c152-4cc8-f23f-f9d46d1e1628"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 13.32 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5998 has 13.65 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 4.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-50719dee895a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidating_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.32 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.09 GiB is free. Process 5998 has 13.65 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 4.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.shape, validating_data.shape, testing_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "TQGQ7lWmezdm",
        "outputId": "7963c34e-f858-42c4-81ca-003a0bba1aa6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-020a93054032>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidating_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector = embedding_layer.weight"
      ],
      "metadata": {
        "id": "nOK0L_B30Px1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = word_vector.clone().detach()"
      ],
      "metadata": {
        "id": "71AsAXn-hkNu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = {'vocab':vocab,\n",
        "         'word_vectors':word_vectors}\n",
        "\n",
        "torch.save(words, 'words.pt')"
      ],
      "metadata": {
        "id": "AJdAMq9dfUo2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Same for validation and testing data"
      ],
      "metadata": {
        "id": "ZxwNzlrvZfCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data, val_data, val_target, vocab = preprocessing(text, 1000, 85, vocab)"
      ],
      "metadata": {
        "id": "NHwe--_lpgPQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data.shape, val_data.shape, val_target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwLGKS-TqMB-",
        "outputId": "1faa34f3-90d6-4a56-ac40-bb3aead6b023"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27932,), torch.Size([13966, 1000]), torch.Size([13966, 85]))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igV_goxTqojm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}