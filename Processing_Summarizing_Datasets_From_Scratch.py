# -*- coding: utf-8 -*-
"""Processing_summarizing_datasets_from_scratch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/osamaakhaled/processing-summarizing-datasets-from-scratch.036d53d1-762c-4b01-bbfc-e6a448e57311.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250601/auto/storage/goog4_request%26X-Goog-Date%3D20250601T125116Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6b0c67fc312c56ab743cd1143ecfe2844df29e1d66172ff6c5eedde5f1a372f2e0d991d4b1377edde283b87f9247a7b553a1039ad59051ac84ed5b26582181d45acce89ce9f3cd76eef9ab540aa710ab01b1cf2c0c7080b26a26bf2294aa36a847777d5a61b1b2e1ba69a003c7fa07307d1fe856ef68fcd09f67321b7bb3b880b65a29d41620464b8bd2cb93ed866683ddd9954204e1063c404a4df417c41dd684126f45f54639469d28edbb130819964924e32d7ed07c8c2f7f379e43ddf70d3f6966f72ce9ce8fc5a56acfea0673d3a55b85d244b595fd71e2864b9bd0084c548a7fa1b92bf88cc43cdf033a04a58f23560187561ef269a566310c34a99962
"""

import torch
import torch.nn as nn
import numpy as np
from datasets import load_dataset
from string import punctuation

"""# Pre-Processing"""

def removing_unwanted_characters(text):
    punctuations = punctuation.replace("'","")
    punctuations = punctuation.replace(".","")
    punctuations = punctuation.replace(",","")
    punctuations += 'â€¢1234567890'

    text = "".join([c for c in text if c not in punctuations])
    text = text.split("\n")
    text = " ".join(text)
    text = text.strip()
    text = " ".join(text.split())

    new_text = str(text[0])
    for char in range(1, len(text)):
        if (text[char] == '.' or text[char] == ',') and new_text[-1] != ' ':
            new_text += ' '
            new_text += text[char]
        elif (new_text[-1] == '.' or new_text[-1] == ',') and text[char] != ' ':
            new_text += ' '
            new_text += text[char]
        else:
            new_text += text[char]

    return new_text

def tokenizing(data):
    all_unique_words = []
    for text in data:
        all_unique_words.extend(text.split())
    vocab = set(all_unique_words)
    vocab = {word:ii for ii, word in enumerate(vocab, 3)}# '<pad>': 0, '<bos>': 1, '<eos>': 2

    didicted_tokens = ['<pad>', '<bos>', '<eos>']
    for i in range(3):
        vocab[didicted_tokens[i]] = i

    return vocab

def convert_words_to_tokens(data, vocab):
    data_ints = []

    for text in data:
        data_ints.append([vocab.get(word) for word in text.split() if vocab.get(word) != None])
        data_ints[-1].insert(0,1)
        data_ints[-1].append(2)

    size = int(data.shape[0] // 2)
    input_data = data_ints[:size]
    target_data = data_ints[size:]

    return input_data, target_data

def remove_short_articles(data, input_data, target_data):
    small_lenghts_indices = [index for index, text in enumerate(input_data) if len(text) < 100]
    small_lenghts_indices = small_lenghts_indices[::-1]
    for index in small_lenghts_indices:
         input_data.pop(index)
         target_data.pop(index)

    size = int(data.shape[0] // 2)
    small_lenghts_indices_targets = [length + size for length in small_lenghts_indices]
    small_lenghts_indices = small_lenghts_indices + small_lenghts_indices_targets
    data = np.delete(data, small_lenghts_indices, axis=0)

    return data, input_data, target_data

def paddings(input_data, length):
    size = len(input_data)
    for i in range(size):
        if len(input_data[i]) < length:
            rem = length - len(input_data[i])
            input_data[i].extend([0]*rem)

        elif len(input_data[i]) > length:
            input_data[i] = input_data[i][:length]

    return input_data

def preprocessing(data, length_data, length_target, vocab):
    keys = list(data.keys())
    data = np.concatenate((data.get(keys[0]), data.get(keys[1])))
    data = np.array([text.lower() for text in data])
    items = data.shape[0]

    for i in range(items):
       data[i] += " ."
       data[i] = removing_unwanted_characters(data[i])
    if not vocab:
        vocab = tokenizing(data)

    input_data, target_data = convert_words_to_tokens(data, vocab)

    data, input_data, target_data = remove_short_articles(data, input_data, target_data)

    input_data = paddings(input_data, length_data)
    target_data = paddings(target_data, length_target)

    return data, torch.tensor(input_data), torch.tensor(target_data), vocab


def splitting(input_data, target_data, split_frac):
    items = split_frac * input_data.shape[0]
    training_input = input_data[:items]
    training_target = target_data[:items]

    rest = input_data.shape[0] - items

    val_set = input_data[items:items+rest//2]
    test_set = input_data[items+rest//2:] 